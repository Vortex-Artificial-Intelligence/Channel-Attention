{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db341754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SAM(nn.Module):\n",
    "    def __init__(self, bias=False):\n",
    "        super(SAM, self).__init__()\n",
    "        self.bias = bias\n",
    "        self.conv = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=7, stride=1, padding=3, dilation=1, bias=self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        max = torch.max(x,1)[0].unsqueeze(1)\n",
    "        avg = torch.mean(x,1).unsqueeze(1)\n",
    "        concat = torch.cat((max,avg), dim=1)\n",
    "        output = self.conv(concat)\n",
    "        output = F.sigmoid(output) * x \n",
    "        return output\n",
    "\n",
    "class SpatialAttention2D(nn.Module):\n",
    "\n",
    "    def __init__(self, kernel_size: Optional[int] = 7, bias: Optional[bool] = False) -> None:\n",
    "        super().__init__()\n",
    "        assert kernel_size % 2 == 1, \"The kernel size must be odd.\"\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = 1\n",
    "        self.padding = kernel_size // 2\n",
    "        self.bias = bias\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=kernel_size, stride=self.stride, padding=self.padding, dilation=1, bias=self.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        max = torch.max(x,1)[0].unsqueeze(1)\n",
    "        avg = torch.mean(x,1).unsqueeze(1)\n",
    "        concat = torch.cat((max,avg), dim=1)\n",
    "        output = self.conv(concat)\n",
    "        output = F.sigmoid(output) * x \n",
    "        return output\n",
    "    \n",
    "    \n",
    "class SpatialAttention1D(nn.Module):\n",
    "\n",
    "    def __init__(self, kernel_size: Optional[int] = 7, bias: Optional[bool] = False) -> None:\n",
    "        super().__init__()\n",
    "        assert kernel_size % 2 == 1, \"The kernel size must be odd.\"\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = 1\n",
    "        self.padding = kernel_size // 2\n",
    "        self.bias = bias\n",
    "        \n",
    "        self.conv = nn.Conv1d(in_channels=2, out_channels=1, kernel_size=kernel_size, stride=self.stride, padding=self.padding, dilation=1, bias=self.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        max = torch.max(x,1)[0].unsqueeze(1)\n",
    "        avg = torch.mean(x,1).unsqueeze(1)\n",
    "        concat = torch.cat((max,avg), dim=1)\n",
    "        output = self.conv(concat)\n",
    "        output = F.sigmoid(output) * x \n",
    "        return output\n",
    "    \n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_dims: int, kernel_size: Optional[int] = 7, bias: Optional[bool] = False) -> None:\n",
    "        super().__init__()\n",
    "        assert n_dims in [1, 2], \"The dimension of input data must be either 1 or 2.\"\n",
    "        \n",
    "        assert kernel_size % 2 == 1, \"The kernel size must be odd.\"\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = 1\n",
    "        self.padding = kernel_size // 2\n",
    "        self.bias = bias\n",
    "        \n",
    "        # parameters for 1D conv\n",
    "        parameters = {\n",
    "            'in_channels': 2,\n",
    "            'out_channels': 1,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'stride': self.stride,\n",
    "            'padding': self.padding,\n",
    "            'dilation': 1,\n",
    "            'bias': self.bias\n",
    "        }\n",
    "\n",
    "        self.conv = nn.Conv1d(**parameters) if n_dims == 1 else nn.Conv2d(**parameters)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        max = torch.max(x,1)[0].unsqueeze(1)\n",
    "        avg = torch.mean(x,1).unsqueeze(1)\n",
    "        concat = torch.cat((max,avg), dim=1)\n",
    "        output = self.conv(concat)\n",
    "        output = F.sigmoid(output) * x \n",
    "        return output\n",
    "    \n",
    "\n",
    "class CAM(nn.Module):\n",
    "    def __init__(self, channels, r):\n",
    "        super(CAM, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.r = r\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=self.channels, out_features=self.channels//self.r, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=self.channels//self.r, out_features=self.channels, bias=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        max = F.adaptive_max_pool2d(x, output_size=1)\n",
    "        avg = F.adaptive_avg_pool2d(x, output_size=1)\n",
    "        b, c, _, _ = x.size()\n",
    "        linear_max = self.linear(max.view(b,c)).view(b, c, 1, 1)\n",
    "        linear_avg = self.linear(avg.view(b,c)).view(b, c, 1, 1)\n",
    "        output = linear_max + linear_avg\n",
    "        output = F.sigmoid(output) * x\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class ChannelAttention2D(nn.Module):\n",
    "    def __init__(self, n_channels: int, r):\n",
    "        super().__init__()\n",
    "        self.channels = n_channels\n",
    "        self.r = r\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(in_features=self.channels, out_features=self.channels//self.r, bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=self.channels//self.r, out_features=self.channels, bias=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        max = F.adaptive_max_pool2d(x, output_size=1)\n",
    "        avg = F.adaptive_avg_pool2d(x, output_size=1)\n",
    "        b, c, _, _ = x.size()\n",
    "        linear_max = self.linear(max.view(b,c)).view(b, c, 1, 1)\n",
    "        linear_avg = self.linear(avg.view(b,c)).view(b, c, 1, 1)\n",
    "        output = linear_max + linear_avg\n",
    "        output = F.sigmoid(output) * x\n",
    "        return output\n",
    "    \n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, r):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.r = r\n",
    "        self.sam = SAM(bias=False)\n",
    "        self.cam = CAM(channels=self.channels, r=self.r)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.cam(x)\n",
    "        output = self.sam(output)\n",
    "        return output + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0eca12b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 224, 224])\n",
      "torch.Size([10, 3, 224])\n"
     ]
    }
   ],
   "source": [
    "sam = SpatialAttention(n_dims=2)\n",
    "x = torch.rand(size=(10, 3, 224, 224))\n",
    "\n",
    "x = sam(x)\n",
    "print(x.shape)\n",
    "\n",
    "sam = SpatialAttention(n_dims=1)\n",
    "x = torch.rand(size=(10, 3, 224))\n",
    "x = sam(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "500e91fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 224])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam = SpatialAttention1D()\n",
    "x = torch.rand(size=(10, 3, 224))\n",
    "\n",
    "x = sam(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fe55d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 16])\n",
      "torch.Size([16, 16, 1, 1])\n",
      "torch.Size([16, 16, 224, 224])\n",
      "torch.Size([16, 16])\n",
      "torch.Size([16, 16, 1])\n",
      "torch.Size([16, 16, 224])\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class SEAttention1D(nn.Module):\n",
    "    \"\"\"\n",
    "    1D Squeeze-and-Excitation Attention for Time Series Analysis.\n",
    "    This module adaptively recalibrates channel-wise feature responses by explicitly modeling interdependencies between channels.\n",
    "    Reference: \"Squeeze-and-Excitation Networks\" by Jie Hu, Li Shen, et al.\n",
    "    URL: https://arxiv.org/abs/1709.01507\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, n_channels: int, reduction: Optional[int] = 8, bias: bool = False\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        :param n_channels: (int) The number of input channels of time series data.\n",
    "        :param reduction: (int) The reduction ratio for the intermediate layer in the SE block.\n",
    "        :param bias: (bool) Whether to include bias terms in the linear layers.\n",
    "        \"\"\"\n",
    "        super(SEAttention1D, self).__init__()\n",
    "        # Global average pooling layer to squeeze the temporal dimension\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Fully connected layers for the excitation operation\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_channels, n_channels // reduction, bias=bias),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(n_channels // reduction, n_channels, bias=bias),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the SEAttention module.\n",
    "\n",
    "        :param x: (torch.Tensor) Input tensor of shape (batch_size, channels, seq_len)\n",
    "\n",
    "        :return: (torch.Tensor) Output tensor of the same shape as input\n",
    "        \"\"\"\n",
    "        # Get the batch size, number of channels, and sequence length\n",
    "        batch_size, channels, _ = x.size()\n",
    "\n",
    "        # Perform the Squeeze operation\n",
    "        y = self.avg_pool(x).view(batch_size, channels)\n",
    "\n",
    "        # Perform the Excitation operation\n",
    "        y = self.fc(y).view(batch_size, channels, 1)\n",
    "\n",
    "        # Scale the input tensor with the recalibrated weights\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "\n",
    "class SEAttention2D(nn.Module):\n",
    "    \"\"\"\n",
    "    2D Squeeze-and-Excitation Attention for Image Analysis.\n",
    "    This module adaptively recalibrates channel-wise feature responses by explicitly modeling interdependencies between channels.\n",
    "    Reference: \"Squeeze-and-Excitation Networks\" by Jie Hu, Li Shen, et al.\n",
    "    URL: https://arxiv.org/abs/1709.01507\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, n_channels: int, reduction: Optional[int] = 4, bias: bool = False\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        :param n_channels: (int) The number of input channels of time series data.\n",
    "        :param reduction: (int) The reduction ratio for the intermediate layer in the SE block.\n",
    "        :param bias: (bool) Whether to include bias terms in the linear layers.\n",
    "        \"\"\"\n",
    "        super(SEAttention2D, self).__init__()\n",
    "        # Global average pooling layer to squeeze the spatial dimensions\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Fully connected layers for the excitation operation\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_channels, n_channels // reduction, bias=bias),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(n_channels // reduction, n_channels, bias=bias),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the SEAttention module.\n",
    "\n",
    "        :param x: (torch.Tensor) Input tensor of shape (batch_size, channels, seq_len)\n",
    "\n",
    "        :return: (torch.Tensor) Output tensor of the same shape as input\n",
    "        \"\"\"\n",
    "        # Get the batch size, number of channels\n",
    "        batch_size, channels, _, _ = x.size()\n",
    "\n",
    "        # Perform the Squeeze operation\n",
    "        y = self.avg_pool(x).view(batch_size, channels)\n",
    "        print(y.size())\n",
    "\n",
    "        # Perform the Excitation operation\n",
    "        y = self.fc(y).view(batch_size, channels, *(1, 1))\n",
    "        print(y.size())\n",
    "\n",
    "        # Scale the input tensor with the recalibrated weights\n",
    "        return x * y.expand_as(x)\n",
    "    \n",
    "    \n",
    "class SEAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    The Squeeze-and-Excitation Attention for Time Series (1D) or Image (2D) Analysis.\n",
    "    This module adaptively recalibrates channel-wise feature responses by explicitly modeling interdependencies between channels.\n",
    "    Reference: \"Squeeze-and-Excitation Networks\" by Jie Hu, Li Shen, et al.\n",
    "    URL: https://arxiv.org/abs/1709.01507\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, n_dims: int, n_channels: int, reduction: Optional[int] = 4, bias: bool = False\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        1D Squeeze-and-Excitation Attention for Time Series Analysis or\n",
    "        2D Squeeze-and-Excitation Attention for Image Analysis.\n",
    "        \n",
    "        :param n_dims: (int) The dimension of input data, either 1 (time series) or 2 (image).\n",
    "        :param n_channels: (int) The number of input channels of time series data.\n",
    "        :param reduction: (int) The reduction ratio for the intermediate layer in the SE block.\n",
    "        :param bias: (bool) Whether to include bias terms in the linear layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Validate the input dimension\n",
    "        assert n_dims in [1, 2], \"The dimension of input data must be either 1 or 2.\"\n",
    "        \n",
    "        # The dimension of inputs data\n",
    "        self.n_dims = n_dims\n",
    "        \n",
    "        # Global average pooling layer to squeeze the spatial dimensions\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1) if n_dims == 2 else nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Fully connected layers for the excitation operation\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_channels, n_channels // reduction, bias=bias),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(n_channels // reduction, n_channels, bias=bias),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        # View shape for reshaping the excitation output\n",
    "        self.view_shape = (1, 1) if n_dims == 2 else (1,)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the SEAttention module.\n",
    "\n",
    "        :param x: (torch.Tensor) \n",
    "                  1D Time Series: Input tensor of shape (batch_size, channels, seq_len);\n",
    "                  2D Image: Input tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "        :return: (torch.Tensor) Output tensor of the same shape as input\n",
    "        \"\"\"\n",
    "        # Get the batch size, number of channels\n",
    "        batch_size, channels = x.size()[:2]\n",
    "\n",
    "        # Perform the Squeeze operation\n",
    "        y = self.avg_pool(x).view(batch_size, channels)\n",
    "\n",
    "        # Perform the Excitation operation\n",
    "        y = self.fc(y).view(batch_size, channels, *self.view_shape)\n",
    "\n",
    "        # Scale the input tensor with the recalibrated weights\n",
    "        return x * y.expand_as(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "x = torch.rand(size=(16, 16, 224, 224))\n",
    "\n",
    "se = SEAttention(n_dims=2, n_channels=16)\n",
    "print(se(x).shape)\n",
    "\n",
    "x = torch.rand(size=(16, 16, 224))\n",
    "se = SEAttention(n_dims=1, n_channels=16)\n",
    "print(se(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5969eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c08b5cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 16, 224, 224])\n",
      "torch.Size([16, 16, 224])\n"
     ]
    }
   ],
   "source": [
    "class ChannelAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, n_dims: int, n_channels: int, reduction: Optional[int] = 4) -> None:\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\" \n",
    "        super().__init__()\n",
    "        \n",
    "        assert n_dims in [1, 2], \"The dimension of input data must be either 1 or 2.\"\n",
    "\n",
    "        self.n_channels = n_channels\n",
    "        self.reduction = reduction\n",
    "        \n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=self.n_channels,\n",
    "                out_features=self.n_channels // self.reduction,\n",
    "                bias=True,\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(\n",
    "                in_features=self.n_channels // self.reduction,\n",
    "                out_features=self.n_channels,\n",
    "                bias=True,\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        # View shape for reshaping the excitation output\n",
    "        self.view_shape = (1, 1) if n_dims == 2 else (1,)\n",
    "        \n",
    "        self.adaptive_max_pool = nn.AdaptiveMaxPool2d(1) if n_dims == 2 else nn.AdaptiveMaxPool1d(1)\n",
    "        self.adaptive_avg_pool = nn.AdaptiveAvgPool2d(1) if n_dims == 2 else nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        max = self.adaptive_max_pool(x)\n",
    "        avg = self.adaptive_avg_pool(x)\n",
    "        \n",
    "        batch_size, n_channels = x.size()[:2]\n",
    "        linear_max = self.linear(max.view(batch_size, n_channels)).view(batch_size, n_channels, *self.view_shape)\n",
    "        linear_avg = self.linear(avg.view(batch_size, n_channels)).view(batch_size, n_channels, *self.view_shape)\n",
    "        \n",
    "        output = linear_max + linear_avg\n",
    "        \n",
    "        output = F.sigmoid(output) * x\n",
    "        return output\n",
    "    \n",
    "\n",
    "ca = ChannelAttention(n_dims=2, n_channels=16)\n",
    "x = torch.rand(size=(16, 16, 224, 224))\n",
    "x = ca(x)\n",
    "print(x.shape)\n",
    "\n",
    "ca = ChannelAttention(n_dims=1, n_channels=16)\n",
    "x = torch.rand(size=(16, 16, 224))\n",
    "x = ca(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c8e28c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 16, 224, 224])\n",
      "torch.Size([16, 16, 224])\n"
     ]
    }
   ],
   "source": [
    "class ConvBlockAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Block Attention Module (CBAM) for Time Series (1D) or Image (2D) Analysis.\n",
    "    This module sequentially applies Channel Attention and Spatial Attention to refine feature representations.\n",
    "    \n",
    "    Reference: \"CBAM: Convolutional Block Attention Module\" by Sanghyun Woo, et al.\n",
    "    \n",
    "    URL: https://arxiv.org/abs/1807.06521\n",
    "    \n",
    "    Also see: `ChannelAttention` and `SpatialAttention` classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_dims: int, n_channels: int, reduction: Optional[int] = 4, kernel_size: Optional[int] = 7) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Convolutional Block Attention Module.\n",
    "        \n",
    "        :param n_dims: (int) The dimension of input data, either 1 (time series) or 2 (image).\n",
    "        :param n_channels: (int) The number of input channels of time series or image.\n",
    "        :param reduction: (int) The reduction ratio for the intermediate layer in the channel attention block.\n",
    "        :param kernel_size: (int) The size of the convolutional kernel in the spatial attention block. Must be odd to maintain spatial dimensions.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initialize Channel Attention and Spatial Attention modules\n",
    "        self.channel_attention = ChannelAttention(n_dims=n_dims, n_channels=n_channels, reduction=reduction)\n",
    "        self.spatial_attention = SpatialAttention(n_dims=n_dims, kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, with_residual: bool = True) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the Convolutional Block Attention Module.\n",
    "        \n",
    "        :param x: (torch.Tensor)\n",
    "                  1D Time Series: Input tensor of shape (batch_size, channels, seq_len);\n",
    "                  2D Image: Input tensor of shape (batch_size, channels, height, width).\n",
    "        :param with_residual: (bool) Whether to include a residual connection from input to output.\n",
    "                  \n",
    "        :return: (torch.Tensor) Output tensor of the same shape as input.\n",
    "        \"\"\"\n",
    "        output = self.channel_attention(x)\n",
    "        output = self.spatial_attention(output)\n",
    "        \n",
    "        if with_residual:\n",
    "            return output + x\n",
    "        return output\n",
    "    \n",
    "    \n",
    "cbam = ConvBlockAttention(n_dims=2, n_channels=16)\n",
    "x = torch.rand(size=(16, 16, 224, 224))\n",
    "x = cbam(x)\n",
    "print(x.shape)\n",
    "\n",
    "cbam = ConvBlockAttention(n_dims=1, n_channels=16)\n",
    "x = torch.rand(size=(16, 16, 224))\n",
    "x = cbam(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6801f1a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timesnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
