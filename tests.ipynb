{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa232add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 论文：FECAM: Frequency Enhanced Channel Attention Mechanism for Time Series Forecasting\n",
    "# 论文地址：https://arxiv.org/abs/2212.01209\n",
    "\n",
    "try:\n",
    "    from torch.fft import rfft, irfft\n",
    "except ImportError:\n",
    "\n",
    "    def rfft(x, d):\n",
    "        t = torch.fft.fft(x, dim=(-d))\n",
    "        r = torch.stack((t.real, t.imag), -1)\n",
    "        return r\n",
    "\n",
    "    def irfft(x, d):\n",
    "        t = torch.fft.ifft(torch.complex(x[:, :, 0], x[:, :, 1]), dim=(-d))\n",
    "        return t.real\n",
    "\n",
    "\n",
    "def dct(x, norm=None):\n",
    "    \"\"\"\n",
    "    Discrete Cosine Transform, Type II (a.k.a. the DCT)\n",
    "\n",
    "    For the meaning of the parameter `norm`, see:\n",
    "    https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.fftpack.dct.html\n",
    "\n",
    "    :param x: the input signal\n",
    "    :param norm: the normalization, None or 'ortho'\n",
    "    :return: the DCT-II of the signal over the last dimension\n",
    "    \"\"\"\n",
    "    x_shape = x.shape\n",
    "    N = x_shape[-1]\n",
    "    x = x.contiguous().view(-1, N)\n",
    "\n",
    "    v = torch.cat([x[:, ::2], x[:, 1::2].flip([1])], dim=1)\n",
    "\n",
    "    Vc = rfft(v, 1)\n",
    "\n",
    "    k = -torch.arange(N, dtype=x.dtype, device=x.device)[None, :] * np.pi / (2 * N)\n",
    "    W_r = torch.cos(k)\n",
    "    W_i = torch.sin(k)\n",
    "\n",
    "    V = Vc[:, :, 0] * W_r - Vc[:, :, 1] * W_i\n",
    "\n",
    "    if norm == \"ortho\":\n",
    "        V[:, 0] /= np.sqrt(N) * 2\n",
    "        V[:, 1:] /= np.sqrt(N / 2) * 2\n",
    "\n",
    "    V = 2 * V.view(*x_shape)\n",
    "\n",
    "    return V\n",
    "\n",
    "\n",
    "class FreqEnhancedAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_dims: int,\n",
    "        n_channels: int,\n",
    "        reduction: Optional[int] = 2,\n",
    "        dropout: Optional[float] = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_channels, n_channels // reduction, bias=False),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(n_channels // reduction, n_channels, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        self.dct_norm = nn.LayerNorm(n_channels, eps=1e-6)  # for lstm on length-wise\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, l = x.size()  # (B,C,L) (32,96,512)\n",
    "        list = []\n",
    "        for i in range(c):\n",
    "            freq = dct(x[:, i, :])\n",
    "            list.append(freq)\n",
    "\n",
    "        stack_dct = torch.stack(list, dim=1)\n",
    "\n",
    "        lr_weight = F.normalize()\n",
    "        lr_weight = self.dct_norm(stack_dct)\n",
    "        lr_weight = self.fc(lr_weight)\n",
    "        lr_weight = self.dct_norm(lr_weight)\n",
    "\n",
    "        return x * lr_weight\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input = torch.rand(8, 7, 96)\n",
    "    block = FreqEnhancedAttention(96)\n",
    "    result = block(input)\n",
    "    print(\"input_tensor.shape:\", input.shape)\n",
    "    print(\"result.shape:\", result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e95406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Mix(nn.Module):\n",
    "    def __init__(self, m: float = -0.80) -> None:\n",
    "        super(Mix, self).__init__()\n",
    "        w = torch.nn.Parameter(torch.FloatTensor([m]), requires_grad=True)\n",
    "        w = torch.nn.Parameter(w, requires_grad=True)\n",
    "        self.w = w\n",
    "        self.mix_block = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, feature1: torch.Tensor, feature2: torch.Tensor) -> torch.Tensor:\n",
    "        mix_factor = self.mix_block(self.w)\n",
    "        out = feature1 * mix_factor.expand_as(feature1) + feature2 * (\n",
    "            1 - mix_factor.expand_as(feature2)\n",
    "        )\n",
    "        return out\n",
    "\n",
    "\n",
    "#\n",
    "class FineGrainedCAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Adaptive Fine-Grained Channel Attention (FCA) module for Time Series or Image Data.\n",
    "    This module captures fine-grained cross-channel interaction adaptively.\n",
    "\n",
    "    Reference: Unsupervised Bidirectional Contrastive Reconstruction and Adaptive Fine-Grained Channel Attention Networks for image dehazing\n",
    "\n",
    "    URL: https://www.sciencedirect.com/science/article/abs/pii/S0893608024002387\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_dims: int,\n",
    "        n_channels: int,\n",
    "        b: float = 1.0,\n",
    "        gamma: float = 2.0,\n",
    "        bias: Optional[bool] = False,\n",
    "    ) -> None:\n",
    "        super(FineGrainedCAttention, self).__init__()\n",
    "\n",
    "        # Dimension assertion\n",
    "        assert n_dims in [1, 2], \"The dimension of input data must be either 1 or 2.\"\n",
    "        self.n_dims = n_dims\n",
    "\n",
    "        # Create the adaptive fine-grained channel attention module\n",
    "        self.avg_pool = (\n",
    "            nn.AdaptiveAvgPool2d(1) if n_dims == 2 else nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "\n",
    "        # 一维卷积\n",
    "        t = int(abs((math.log(n_channels, 2) + b) / gamma))\n",
    "        kernal_size = t if t % 2 else t + 1\n",
    "\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            1, 1, kernel_size=kernal_size, padding=kernal_size // 2, bias=bias\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Conv2d(n_channels, n_channels, 1, padding=0, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.mix = Mix()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pool = self.avg_pool(x)\n",
    "        x1 = self.conv1(pool.squeeze(-1).transpose(-1, -2)).transpose(\n",
    "            -1, -2\n",
    "        )  # (1,64,1)\n",
    "\n",
    "        print(\"x1:\", x1.size())\n",
    "\n",
    "        x2 = self.fc(pool).squeeze(-1).transpose(-1, -2)  # (1,1,64)\n",
    "        print(\"x2:\", x2.size())\n",
    "\n",
    "        out1 = (\n",
    "            torch.sum(torch.matmul(x1, x2), dim=1).unsqueeze(-1).unsqueeze(-1)\n",
    "        )  # (1,64,1,1)\n",
    "        out1 = self.sigmoid(out1)\n",
    "        out2 = (\n",
    "            torch.sum(torch.matmul(x2.transpose(-1, -2), x1.transpose(-1, -2)), dim=1)\n",
    "            .unsqueeze(-1)\n",
    "            .unsqueeze(-1)\n",
    "        )\n",
    "\n",
    "        out2 = self.sigmoid(out2)\n",
    "        out = self.mix(out1, out2)\n",
    "        out = (\n",
    "            self.conv1(out.squeeze(-1).transpose(-1, -2))\n",
    "            .transpose(-1, -2)\n",
    "            .unsqueeze(-1)\n",
    "        )\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return x * out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input = torch.rand(1, 64, 256, 256)\n",
    "    block = FineGrainedCAttention(n_dims=2, n_channels=64)\n",
    "    output = block(input)\n",
    "    print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128bea76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_y: torch.Size([1, 64, 1, 1])\n",
      "style_assignment: torch.Size([1, 3])\n",
      "avg_y repeated: torch.Size([1, 192, 1, 1])\n",
      "z: torch.Size([1, 192, 1, 1])\n",
      "torch.Size([1, 64, 256, 256])\n",
      "avg_y: torch.Size([1, 64, 1])\n",
      "style_assignment: torch.Size([1, 3])\n",
      "avg_y repeated: torch.Size([1, 192, 1])\n",
      "z: torch.Size([1, 192, 1])\n",
      "torch.Size([1, 64, 256])\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from channel_attention.utils import create_conv_layer\n",
    "\n",
    "\n",
    "class SEAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    The Squeeze-and-Excitation Attention for Time Series (1D) or Image (2D) Analysis.\n",
    "    This module adaptively recalibrates channel-wise feature responses by explicitly modeling interdependencies between channels.\n",
    "\n",
    "    Reference: \"Squeeze-and-Excitation Networks\" by Jie Hu, Li Shen, et al.\n",
    "\n",
    "    URL: https://arxiv.org/abs/1709.01507\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_dims: int,\n",
    "        n_channels: int,\n",
    "        reduction: Optional[int] = 4,\n",
    "        bias: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        1D Squeeze-and-Excitation Attention for Time Series Analysis or\n",
    "        2D Squeeze-and-Excitation Attention for Image Analysis.\n",
    "\n",
    "        :param n_dims: (int) The dimension of input data, either 1 (time series) or 2 (image).\n",
    "        :param n_channels: (int) The number of input channels of time series data.\n",
    "        :param reduction: (int) The reduction ratio for the intermediate layer in the SE block.\n",
    "        :param bias: (bool) Whether to include bias terms in the linear layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Validate the input dimension\n",
    "        assert n_dims in [1, 2], \"The dimension of input data must be either 1 or 2.\"\n",
    "\n",
    "        # The dimension of inputs data\n",
    "        self.n_dims = n_dims\n",
    "\n",
    "        # Global average pooling layer to squeeze the spatial dimensions\n",
    "        self.avg_pool = (\n",
    "            nn.AdaptiveAvgPool2d(1) if n_dims == 2 else nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "\n",
    "        # Fully connected layers for the excitation operation\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_channels, n_channels // reduction, bias=bias),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(n_channels // reduction, n_channels, bias=bias),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        # View shape for reshaping the excitation output\n",
    "        self.view_shape = (1, 1) if n_dims == 2 else (1,)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the SEAttention module.\n",
    "\n",
    "        :param x: (torch.Tensor)\n",
    "                  1D Time Series: Input tensor of shape (batch_size, channels, seq_len);\n",
    "                  2D Image: Input tensor of shape (batch_size, channels, height, width).\n",
    "\n",
    "        :return: (torch.Tensor) Output tensor of the same shape as input\n",
    "        \"\"\"\n",
    "        # Get the batch size, number of channels\n",
    "        batch_size, channels = x.size()[:2]\n",
    "\n",
    "        # Perform the Squeeze operation\n",
    "        y = self.avg_pool(x).view(batch_size, channels)\n",
    "\n",
    "        # Perform the Excitation operation\n",
    "        y = self.fc(y).view(batch_size, channels, *self.view_shape)\n",
    "\n",
    "        # Scale the input tensor with the recalibrated weights\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "\n",
    "class MultiSEAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Branch Squeeze-and-Excitation Attention Module for Time Series (1D) or Image (2D) Analysis.\n",
    "    This module enhances the representational power of the standard SE block by incorporating multiple branches and adaptive style assignment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, n_dims: int, n_channels: int, reduction: int = 4, n_branches: int = 3\n",
    "    ) -> None:\n",
    "        super(MultiSEAttention, self).__init__()\n",
    "\n",
    "        # Dimension assertion\n",
    "        assert n_dims in [1, 2], \"The dimension of input data must be either 1 or 2.\"\n",
    "        self.n_dims = n_dims\n",
    "\n",
    "        # Create the average pooling layer and activation function\n",
    "        self.avg_pool = (\n",
    "            nn.AdaptiveAvgPool2d(1) if n_dims == 2 else nn.AdaptiveAvgPool1d(1)\n",
    "        )\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "        # Store the reduction ratio, number of branches, and number of channels\n",
    "        self.reduction = reduction\n",
    "        self.n_branches = n_branches\n",
    "        self.n_channels = n_channels\n",
    "        new_channels = n_channels * n_branches\n",
    "\n",
    "        # Layers for multi-branch excitation\n",
    "        self.fc = nn.Sequential(\n",
    "            create_conv_layer(\n",
    "                n_dims=n_dims,\n",
    "                in_channels=new_channels,\n",
    "                out_channels=new_channels // self.reduction,\n",
    "                kernel_size=1,\n",
    "                bias=True,\n",
    "                groups=n_branches,\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            create_conv_layer(\n",
    "                n_dims=n_dims,\n",
    "                in_channels=new_channels // self.reduction,\n",
    "                out_channels=new_channels,\n",
    "                kernel_size=1,\n",
    "                bias=True,\n",
    "                groups=n_branches,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Style assignment layer\n",
    "        self.style_assigner = nn.Linear(n_channels, n_branches, bias=False)\n",
    "\n",
    "        # Repeat size for reshaping the output\n",
    "        self.repeat_size = (1, 1) if n_dims == 2 else (1,)\n",
    "\n",
    "    def _style_assignment(\n",
    "        self, channel_mean: torch.Tensor, batch_size: int\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Assign styles to each channel based on the channel mean.\n",
    "\n",
    "        :param channel_mean: (torch.Tensor) The mean values of each channel, shape (batch_size, n_channels, 1, 1).\n",
    "        :param batch_size: (int) The batch size of the input tensor.\n",
    "\n",
    "        :return: (torch.Tensor) Style assignment probabilities for each branch, shape (batch_size, n_branches).\n",
    "        \"\"\"\n",
    "        style_assignment = self.style_assigner(channel_mean.view(batch_size, -1))\n",
    "        style_assignment = nn.functional.softmax(style_assignment, dim=1)\n",
    "        return style_assignment\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        avg_y = self.avg_pool(x)\n",
    "        print(\"avg_y:\", avg_y.size())\n",
    "        batch_size, n_channels = avg_y.shape[:2]\n",
    "\n",
    "        style_assignment = self._style_assignment(avg_y, batch_size=batch_size)  # B x N\n",
    "        avg_y = avg_y.repeat(1, self.n_branches, *self.repeat_size)  # B x NC x 1 x 1\n",
    "        z = self.fc(avg_y)  # B x NC x 1 x 1\n",
    "        style_assignment = style_assignment.repeat_interleave(n_channels, dim=1)\n",
    "\n",
    "        if self.n_dims == 1:\n",
    "            z = z * style_assignment[:, :, None]\n",
    "        else:\n",
    "            z = z * style_assignment[:, :, None, None]\n",
    "\n",
    "        # [batch_size, n_channels, 1, 1]\n",
    "        z = torch.sum(\n",
    "            z.view(batch_size, self.n_branches, n_channels, *self.repeat_size), dim=1\n",
    "        )  # B x C x 1 x 1\n",
    "        z = self.activation(z)\n",
    "\n",
    "        return x * z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88291fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
